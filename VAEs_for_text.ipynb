{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAEs_for_text.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raphaelgbe/Variational-AutoEncoders/blob/master/VAEs_for_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrEv0qf70VsF",
        "colab_type": "text"
      },
      "source": [
        "# VAE for text data trained on PTB dataset with ELBO & MMD objectives (with annealing & embedding dropout):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHRfn0bKM7sD",
        "colab_type": "code",
        "outputId": "cf2c26f6-71a0-4f08-ee7f-61af1d194972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "  import os, sys, math, re\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import torch\n",
        "  import torch.nn as nn\n",
        "  import torch.nn.functional as F\n",
        "  import torchvision\n",
        "  \n",
        "  from collections import Counter\n",
        "  \n",
        "  import random\n",
        "  \n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "  \n",
        "  seed = 4056\n",
        "  random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  \n",
        "  # really helpful points of reference, from which this notebook takes \n",
        "  # inspiration: \n",
        "  # #https://arxiv.org/abs/1511.06349\n",
        "  # #https://github.com/timbmg/Sentence-VAE "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f753fc6bb50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhco9I_FjsaX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "else:\n",
        "  device = torch.device('cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRV5_-kmejw5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_folder = 'drive/My Drive/text_data'\n",
        "\n",
        "def import_text_dataset(path):\n",
        "  \n",
        "  res = []\n",
        "  \n",
        "  with open(path, 'r') as f:\n",
        "    for line in f:\n",
        "      res.append(line)\n",
        "      \n",
        "  return res\n",
        "\n",
        "data_train = import_text_dataset(os.path.join(data_folder, 'ptb.train.txt'))\n",
        "data_val = import_text_dataset(os.path.join(data_folder, 'ptb.valid.txt'))\n",
        "data_test = import_text_dataset(os.path.join(data_folder, 'ptb.test.txt'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yevNdgrfEctQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "other_words = ['<start>', '<end>', '<pad>']\n",
        "vocab = set([x for s in data_train for x in s.split()]).union(set(other_words))\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "vocab.update(set([x for s in data_val for x in s.split()]))\n",
        "vocab.update(set([x for s in data_test for x in s.split()]))\n",
        "\n",
        "total_vocab_size = len(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9TJCzTIhKDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentence_lengths = Counter()\n",
        "\n",
        "sentence_lengths.update(list(map(len, data_train + data_val + data_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbwOn_UyiRqH",
        "colab_type": "code",
        "outputId": "81e8915d-54e8-449b-b24d-dde56183f967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(len(data_train + data_val + data_test))\n",
        "\n",
        "# quick statistics of sentence lengths to determine a good max_length\n",
        "print(sum([v for k, v in sentence_lengths.items() if k >= 200]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "49199\n",
            "4795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r35-OhiaQj96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "lr = 0.001\n",
        "epochs = 10\n",
        "\n",
        "loss_type = 'elbo' # or 'mmd'\n",
        "anneal_function = 'logistic'\n",
        "anneal_coeff = 0.0025\n",
        "anneal_bias = 4000#2500\n",
        "variational_coeff = 0.5\n",
        "\n",
        "z_dim = 16\n",
        "embedding_dim = 300\n",
        "embedding_dropout = 0.5\n",
        "hidden_dim = 256\n",
        "\n",
        "max_sequence_length = 60 #200\n",
        "min_occurrences = 1\n",
        "\n",
        "word_dropout = 0.\n",
        "\n",
        "# intiialy only implemented LSTM but might be interesting to look at other options:\n",
        "rnn_type = 'GRU'\n",
        "\n",
        "# four tokens pre-allocated: 0 for UNK, 1 for START, 2 for END and 3 for PAD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPmWphEBUMA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_kernel(x, y):\n",
        "  \n",
        "    x_size = x.shape[0]\n",
        "    y_size = y.shape[0]\n",
        "    dim = x.shape[1]\n",
        "    tiled_x = x.view(x_size, 1, dim).repeat(1, y_size, 1)\n",
        "    tiled_y = y.view(1, y_size, dim).repeat(x_size, 1, 1)\n",
        "    return torch.exp(- torch.mean((tiled_x - tiled_y)**2, dim=2) / float(dim))\n",
        "\n",
        "def compute_mmd(x, y):\n",
        "  \n",
        "  x_kernel = compute_kernel(x, x)\n",
        "  y_kernel = compute_kernel(y, y)\n",
        "  xy_kernel = compute_kernel(x, y)\n",
        "  return (x_kernel + y_kernel - 2 * xy_kernel).mean()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX_L6veheJ6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextVAE(nn.Module):\n",
        "  \n",
        "  def __init__(self, embedding_dim=embedding_dim, hidden_dim=hidden_dim, \n",
        "               z_dim=z_dim, word_dropout=word_dropout, vocab_size=vocab_size, \n",
        "               embedding_dropout = embedding_dropout, min_occ=min_occurrences, \n",
        "               max_len=max_sequence_length, anneal_bias=anneal_bias, \n",
        "               anneal_coeff=anneal_coeff, variational_coeff=variational_coeff, \n",
        "               loss_type=loss_type, anneal_function=anneal_function, \n",
        "               rnn_type=rnn_type, generate_cell_state=False):\n",
        "    \n",
        "    super(TextVAE, self).__init__()\n",
        "    \n",
        "    # Attributes definition:\n",
        "    \n",
        "    self.max_len = max_len\n",
        "    self.min_occ = min_occ\n",
        "    self.word_dropout = word_dropout\n",
        "    self.anneal_coeff = anneal_coeff\n",
        "    self.variational_coeff = variational_coeff\n",
        "    self.loss_type = loss_type\n",
        "    self.anneal_function = anneal_function\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.vocab_size = vocab_size\n",
        "    self.rnn_type = rnn_type\n",
        "    if self.rnn_type == 'LSTM':\n",
        "      self.generate_cell_state = generate_cell_state\n",
        "    \n",
        "    # Layers definition: \n",
        "    \n",
        "    self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "    self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "    \n",
        "    if self.rnn_type == 'LSTM':\n",
        "      self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "      self.decoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "    elif self.rnn_type == 'GRU':\n",
        "      self.encoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "      self.decoder = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
        "\n",
        "    \n",
        "    if loss_type == 'elbo':\n",
        "      self.latent_dim = z_dim\n",
        "      self.fc1 = nn.Linear(hidden_dim, self.latent_dim)\n",
        "      self.fc2 = nn.Linear(hidden_dim, self.latent_dim)\n",
        "    elif loss_type == 'mmd':\n",
        "      self.latent_dim = 2 * z_dim\n",
        "      self.fc1 = nn.Linear(hidden_dim, self.latent_dim)\n",
        "    else:\n",
        "      raise ValueError(\"Not a valid loss: choose between 'elbo' and 'mmd'.\")\n",
        "      \n",
        "    self.fc_generate_hidden = nn.Linear(self.latent_dim, hidden_dim)\n",
        "    if self.rnn_type == 'LSTM':\n",
        "      if self.generate_cell_state:\n",
        "        self.fc_generate_cell = nn.Linear(self.latent_dim, hidden_dim)\n",
        "    self.final_layer = nn.Linear(hidden_dim, vocab_size)\n",
        "    \n",
        "    \n",
        "  def forward(self, x, batch_lengths, batch_size=batch_size):\n",
        "    \n",
        "    # On keeping sigmoid activations in LSTMs:\n",
        "    # https://www.quora.com/Why-does-an-LSTM-with-ReLU-activations-diverge\n",
        "    \n",
        "    original_input = x.clone() # needs to be used when decoding\n",
        "    \n",
        "    # Encoding:\n",
        "    \n",
        "    x = self.embedding(x)\n",
        "      \n",
        "    x = self.preprocess_sequential_input(x, batch_lengths)\n",
        "    \n",
        "    # hidden input set to 0:\n",
        "    if self.rnn_type == 'LSTM':\n",
        "      hidden_initial = (torch.zeros(1, batch_size, self.hidden_dim),\n",
        "                            torch.zeros(1, batch_size, self.hidden_dim))\n",
        "    \n",
        "      outputs, hidden = self.encoder(x, hidden_initial)\n",
        "    \n",
        "      hidden = hidden[0] # hidden contains last (hidden_state, cell_state) pair\n",
        "    \n",
        "    else:\n",
        "      hidden_initial = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "    \n",
        "      outputs, hidden = self.encoder(x, hidden_initial)\n",
        "    \n",
        "    \n",
        "    if self.loss_type == 'elbo':\n",
        "      mean = self.fc1(hidden)\n",
        "      logvar = self.fc2(hidden)\n",
        "      z = self.reparametrize(mean, logvar)\n",
        "    elif self.loss_type == 'mmd':\n",
        "      z = self.fc1(hidden) \n",
        "      \n",
        "    # Decoding:\n",
        "    \n",
        "    hidden_state = self.fc_generate_hidden(z)\n",
        "    if self.rnn_type == 'LSTM':\n",
        "      if self.generate_cell_state:\n",
        "        cell_state = self.fc_generate_cell(z)\n",
        "    \n",
        "    \n",
        "    if self.word_dropout:\n",
        "      # the 'words' that are not to be predicted are excluded: START & PAD:\n",
        "      exclusion_condition = original_input.data != 1 or original_input.data != 3\n",
        "      \n",
        "      modif_entries_shape = original_input[exclusion_condition].size()\n",
        "      \n",
        "      dropout_filter = torch.rand(modif_entries_shape)\n",
        "      dropout_filter[dropout_filter.data < self.word_dropout] = 0\n",
        "      dropout_filter[dropout_filter.data >= self.word_dropout] = 1\n",
        "      \n",
        "      original_input[exclusion_condition] = \\\n",
        "         original_input[exclusion_condition].mul(dropout_filter)\n",
        "      \n",
        "    \n",
        "    x = self.embedding(original_input)\n",
        "    \n",
        "    if self.training:\n",
        "      x = self.embedding_dropout(x)\n",
        "      \n",
        "    x = self.preprocess_sequential_input(x, batch_lengths)\n",
        "    \n",
        "    if self.rnn_type == 'LSTM':\n",
        "      if self.generate_cell_state:\n",
        "        hidden = (hidden_state, cell_state)\n",
        "      else:\n",
        "        hidden = (hidden_state, torch.zeros(1, batch_size, self.hidden_dim))\n",
        "    else:\n",
        "      hidden = hidden_state\n",
        "    \n",
        "    \n",
        "    outputs, hidden = self.decoder(x, hidden) \n",
        "    \n",
        "    padded_outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
        "    \n",
        "    seq_len = padded_outputs.size(1)\n",
        "    \n",
        "    results = F.log_softmax(self.final_layer(padded_outputs.contiguous().view(-1, self.hidden_dim)), dim=-1)\n",
        "      \n",
        "    results = results.view(batch_size, seq_len, self.vocab_size)\n",
        "    \n",
        "    if self.training:\n",
        "      if self.loss_type == 'elbo':\n",
        "        return mean, logvar, z, results\n",
        "      elif self.loss_type == 'mmd':\n",
        "        return z, results\n",
        "    else:\n",
        "      return results\n",
        "    \n",
        "  def sample_sentence(self, nb_examples=1, z=None, start_tok=1, end_tok=2, pad_tok=3):\n",
        "    \n",
        "    if z is not None:\n",
        "      nb_examples = z.size(0)\n",
        "    else:\n",
        "      z = torch.randn(nb_examples, self.latent_dim)\n",
        "      \n",
        "    z = z.unsqueeze(0)\n",
        "     \n",
        "    hidden_state = self.fc_generate_hidden(z)\n",
        "    \n",
        "    if self.rnn_type == 'LSTM':\n",
        "      if self.generate_cell_state:\n",
        "        cell_state = self.fc_generate_cell(z)\n",
        "        hidden = (hidden_state, cell_state)\n",
        "      else:\n",
        "        hidden = (hidden_state, torch.zeros(1, nb_examples, self.hidden_dim))\n",
        "    else:\n",
        "      hidden = hidden_state\n",
        "      \n",
        "    t = 0\n",
        "    \n",
        "    input_seq = start_tok * torch.ones(nb_examples, 1)\n",
        "    \n",
        "    res_seq = pad_tok * torch.ones(nb_examples, self.max_len)\n",
        "    \n",
        "    ended_seqs = [0] * nb_examples\n",
        "    \n",
        "    while t < self.max_len and 0 in ended_seqs:\n",
        "      \n",
        "      x = self.embedding(input_seq.long())\n",
        "      \n",
        "      output, hidden = self.decoder(x, hidden)\n",
        "      \n",
        "      results = self.final_layer(output)\n",
        "      \n",
        "      generated_token = torch.max(results, dim=-1)[1][:, 0]\n",
        "      \n",
        "      res_seq[:, t] = generated_token\n",
        "      \n",
        "      if t < self.max_len - 1:\n",
        "        input_seq = generated_token.unsqueeze(1).float()\n",
        "        \n",
        "      ended_seqs += (generated_token == end_tok).tolist()\n",
        "      \n",
        "      t += 1\n",
        "      \n",
        "    return z, res_seq\n",
        "    \n",
        "    \n",
        "  def preprocess_sequential_input(self, input_sequence, batch_lengths):\n",
        "    \n",
        "    \"\"\"\n",
        "    For computational efficiency, we pad and pack the input sequence of the \n",
        "    encoder; note that when using batches, we make sure to sort input sequences\n",
        "    by hand before the forward pass:\n",
        "    https://stackoverflow.com/questions/51030782/why-do-we-pack-the-sequences-in-pytorch\n",
        "    https://discuss.pytorch.org/t/understanding-pack-padded-sequence-and-pad-packed-sequence/4099/4\n",
        "    \"\"\"\n",
        "    \n",
        "    return nn.utils.rnn.pack_padded_sequence(input_sequence, batch_lengths,\n",
        "                                            batch_first = True)\n",
        "    \n",
        "  \n",
        "  def reparametrize(self, mean, logvar, batch_size=batch_size):\n",
        "    \n",
        "    \"\"\"\n",
        "    Reparametrization trick: from mean, variance,and normal noise, we issue a\n",
        "    Gaussian variable of corresponding mean and variance:\n",
        "    https://stats.stackexchange.com/questions/199605/how-does-the-reparameterization-trick-for-vaes-work-and-why-is-it-important\n",
        "    http://stillbreeze.github.io/REINFORCE-vs-Reparameterization-trick/\n",
        "    \"\"\"\n",
        "    \n",
        "    return mean + torch.randn((batch_size, self.latent_dim), \n",
        "                              requires_grad=True) * torch.exp(0.5 * logvar)\n",
        "  \n",
        "  def loss_nll(self, x, x_recon, batch_length):\n",
        "    \n",
        "    # for NLLLoss, we shape input as [-1, vocab_size] and target as [-1]\n",
        "    # first, lengths have to be the same:\n",
        "    x = x[:,:torch.max(batch_lengths).item()]\n",
        "    x = x.contiguous().view(-1)\n",
        "    x_recon = x_recon.contiguous().view(-1, x_recon.size(-1))\n",
        "    \n",
        "    NLL_loss = nn.NLLLoss(ignore_index=3)\n",
        "    \n",
        "    return NLL_loss(x_recon, x)\n",
        "  \n",
        "  def loss_mmd(self, z, batch_size=batch_size):\n",
        "    \n",
        "    true_samples = torch.randn(batch_size, self.latent_dim)\n",
        "    return compute_mmd(true_samples, z.squeeze())\n",
        "    \n",
        "  def loss_elbo(self, mean, logvar):\n",
        "    \n",
        "    return - self.variational_coeff * torch.sum(logvar + 1 \n",
        "                                                - logvar.exp() - mean**2)\n",
        "  \n",
        "  def annealing(self, step,anneal_function=anneal_function, \n",
        "                anneal_coeff=anneal_coeff, anneal_bias=anneal_bias):\n",
        "    \n",
        "    if anneal_function == 'logistic':\n",
        "      return 1 / (1 + np.exp(-anneal_coeff* (step - anneal_bias)))\n",
        "    else:\n",
        "      raise NotImplementedError\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFmpyTICYEw2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "set_of_words = set([word for ls_words in [x.split() for x in data_train] for \n",
        "                    word in ls_words])\n",
        "nb_words = len(set_of_words) + 3 # '<unk>' is in data_train but not <start>,\n",
        "                                 # <end>, <pad>\n",
        "  \n",
        "word2token = OrderedDict({'<unk>': 0, '<start>': 1, '<end>': 2, '<pad>': 3})\n",
        "\n",
        "for word in sorted(list(set_of_words)):\n",
        "  if word != '<unk>':\n",
        "    word2token[word] = len(word2token)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5KK0nRhX1kx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_sentence_to_sequence_of_tokens(sentence, word2token=word2token):\n",
        "  \n",
        "  return [word2token[word] if word in word2token.keys() \n",
        "            else word2token['<unk>'] for word in sentence.split()]\n",
        "\n",
        "def number_to_categorical(nb, vec_size):\n",
        "  \n",
        "  return [0] * nb  + [1] + [0] * (vec_size - 1 - nb)\n",
        "\n",
        "def convert_sentence_to_sequence_of_cat_tokens(sentence, word2token=word2token):\n",
        "  \n",
        "  seq = convert_sentence_to_sequence_of_tokens(sentence, word2token)\n",
        "  return [number_to_categorical(i, len(word2token)) for i in seq]\n",
        "\n",
        "def convert_sequence_of_tokens_to_sentence(seq, word2token=word2token):\n",
        "  \n",
        "  return ' '.join([list(word2token.keys())[token] for token in seq \n",
        "                    if token != word2token['<pad>']])\n",
        "\n",
        "def truncate_or_pad_input(input_sequence, max_len=max_sequence_length, pad_tok=3):\n",
        "  \n",
        "  if len(input_sequence) < max_len:\n",
        "    return input_sequence + [pad_tok] * (max_len - len(input_sequence))\n",
        "  else:\n",
        "    return input_sequence[:max_len]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5k7EvlPtX2SN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_train_tokens = [convert_sentence_to_sequence_of_tokens(s) for s in data_train]\n",
        "data_train_tokens = [truncate_or_pad_input(s) for s in data_train_tokens]\n",
        "\n",
        "data_val_tokens = [convert_sentence_to_sequence_of_tokens(s) for s in data_val]\n",
        "data_val_tokens = [truncate_or_pad_input(s) for s in data_val_tokens]\n",
        "\n",
        "data_test_tokens = [convert_sentence_to_sequence_of_tokens(s) for s in data_test]\n",
        "data_test_tokens = [truncate_or_pad_input(s) for s in data_test_tokens]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJorhLrYikju",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_lists_in_parallel(*ls):\n",
        "  \n",
        "  zipped_ls = list(zip(*ls))\n",
        "  \n",
        "  random.shuffle(zipped_ls)\n",
        "  \n",
        "  return zip(*zipped_ls)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UMn8FMKjstX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(data_train_tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRLfvMmrXjG4",
        "colab_type": "text"
      },
      "source": [
        "ELBO training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SpDnKqGvruYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = TextVAE(rnn_type='LSTM')\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cz-Ik2qlfkKU",
        "colab_type": "code",
        "outputId": "cd34714d-5094-4744-8614-1b01d5063511",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        }
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "nb_batches = len(data_train_tokens)//batch_size\n",
        "pad_tok = word2token['<pad>']\n",
        "start_tok = word2token['<start>']\n",
        "end_tok = word2token['<end>']\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for id_batch in range(nb_batches):\n",
        "        data_batch_as_list = data_train_tokens[(batch_size * id_batch):\n",
        "                                        ((id_batch + 1) * batch_size)]\n",
        "        batch_lengths = [len(seq[:seq.index(pad_tok)]) if pad_tok in seq \n",
        "                              else len(seq) for seq in data_batch_as_list]\n",
        "        batch_lengths = torch.IntTensor(batch_lengths)\n",
        "        sorted_lengths, sorted_idx = torch.sort(batch_lengths, descending=True)\n",
        "        \n",
        "        input_batch = deepcopy(data_batch_as_list)\n",
        "        input_batch = [[start_tok] + seq[:-1] for seq in input_batch]\n",
        "        input_batch = torch.LongTensor(input_batch)\n",
        "        input_batch = input_batch.view(batch_size, max_sequence_length)\n",
        "        input_batch = input_batch[sorted_idx]\n",
        "        \n",
        "        target_batch = deepcopy(data_batch_as_list)\n",
        "        target_batch = [seq[:-1] + [end_tok] if seq[-1] != pad_tok \n",
        "                          else seq[:seq.index(pad_tok)] + [end_tok] + \n",
        "                            seq[seq.index(pad_tok) + 1:] for seq in target_batch]\n",
        "        target_batch = torch.LongTensor(target_batch)\n",
        "        target_batch = target_batch.view(batch_size, max_sequence_length)\n",
        "        target_batch = target_batch[sorted_idx]        \n",
        "        \n",
        "        \n",
        "        mu_batch, logvar_batch,\\\n",
        "            latent_var, data_batch_recon = vae(input_batch, sorted_lengths)\n",
        "        \n",
        "        \n",
        "        elbo_loss = vae.loss_elbo(mu_batch, logvar_batch)\n",
        "        nll_loss = vae.loss_nll(target_batch, data_batch_recon, batch_lengths)\n",
        "        elbo_weight = vae.annealing(id_batch + nb_batches * epoch)\n",
        "        \n",
        "        loss = (nll_loss + elbo_weight * elbo_loss) / batch_size\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if not id_batch % 10:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Batch {id_batch}/{nb_batches}, \\\n",
        "             Loss ELBO: {elbo_loss}; Loss NLL: {nll_loss}, Anneal: {elbo_weight}')\n",
        "            \n",
        "    # save model at each epoch:\n",
        "    torch.save(vae, 'textVAE_elbo_after_epoch_{}_incorp_reparam.pt'.format(epoch))\n",
        "    # https://stackoverflow.com/questions/52277083/pytorch-saving-model-userwarning-couldnt-retrieve-source-code-for-container-of"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10, Batch 0/1314,              Loss ELBO: 3.5825061798095703; Loss NLL: 9.200987815856934, Anneal: 4.5397868702434395e-05\n",
            "Epoch 1/10, Batch 10/1314,              Loss ELBO: 2.9815711975097656; Loss NLL: 9.059290885925293, Anneal: 4.654706772585396e-05\n",
            "Epoch 1/10, Batch 20/1314,              Loss ELBO: 7.619480133056641; Loss NLL: 8.073885917663574, Anneal: 4.772535611847556e-05\n",
            "Epoch 1/10, Batch 30/1314,              Loss ELBO: 128.17884826660156; Loss NLL: 6.9002461433410645, Anneal: 4.893347014058169e-05\n",
            "Epoch 1/10, Batch 40/1314,              Loss ELBO: 39.09267044067383; Loss NLL: 6.901294231414795, Anneal: 5.0172164683764205e-05\n",
            "Epoch 1/10, Batch 50/1314,              Loss ELBO: 9.143732070922852; Loss NLL: 6.731674671173096, Anneal: 5.144221374220898e-05\n",
            "Epoch 1/10, Batch 60/1314,              Loss ELBO: 6.511533737182617; Loss NLL: 6.680471420288086, Anneal: 5.274441089589183e-05\n",
            "Epoch 1/10, Batch 70/1314,              Loss ELBO: 5.466487407684326; Loss NLL: 6.633425712585449, Anneal: 5.407956980598676e-05\n",
            "Epoch 1/10, Batch 80/1314,              Loss ELBO: 4.711821556091309; Loss NLL: 6.7994842529296875, Anneal: 5.5448524722794907e-05\n",
            "Epoch 1/10, Batch 90/1314,              Loss ELBO: 2.0630125999450684; Loss NLL: 6.659796714782715, Anneal: 5.685213100650898e-05\n",
            "Epoch 1/10, Batch 100/1314,              Loss ELBO: 1.744803547859192; Loss NLL: 6.6847615242004395, Anneal: 5.829126566113865e-05\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-1920e8f62c66>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnll_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0melbo_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0melbo_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mid_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RzGU9ryKwu5",
        "colab_type": "code",
        "outputId": "d1e8aa02-abed-497d-c453-0f19a27b469a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1238
        }
      },
      "source": [
        "# results after 4 epochs (posterior collapse is clear in the greedy \n",
        "# sampling of sentences)\n",
        "\n",
        "pad_tok = word2token['<pad>']\n",
        "start_tok = word2token['<start>']\n",
        "end_tok = word2token['<end>']\n",
        "\n",
        "from copy import deepcopy\n",
        "data_batch_as_list = data_train_tokens[:batch_size]\n",
        "batch_lengths = [len(seq[:seq.index(pad_tok)]) if pad_tok in seq \n",
        "                      else seq for seq in data_batch_as_list]\n",
        "batch_lengths = torch.IntTensor(batch_lengths)\n",
        "sorted_lengths, sorted_idx = torch.sort(batch_lengths, descending=True)\n",
        "\n",
        "example_batch = deepcopy(data_batch_as_list)\n",
        "example_batch = [[start_tok] + seq[:-1] for seq in example_batch]\n",
        "example_batch = torch.LongTensor(example_batch)\n",
        "example_batch = example_batch.view(batch_size, max_sequence_length)\n",
        "example_batch = example_batch[sorted_idx]\n",
        "vae.train()\n",
        "for i in range(example_batch.size(0)):\n",
        "  print(i, convert_sequence_of_tokens_to_sentence((torch.max((vae(example_batch, sorted_lengths)[-1]), dim=-1)[1][i]).tolist()),\n",
        "          convert_sequence_of_tokens_to_sentence(example_batch[i].tolist()))\n",
        "list(map(convert_sequence_of_tokens_to_sentence, vae.sample_sentence(z=vae(example_batch, sorted_lengths)[0].squeeze())[1].int()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 the the the the are the is are to in a as <unk> <unk> to year been been a <unk> in <unk> the <unk> of the and <unk> to the <unk> <unk> <unk> <unk> of to the and the of the <unk> <unk> <unk> <unk> <unk> <unk> <start> for journalists however who write what they <unk> view as history 's first draft this has also been a week to give a lot of space and time to ron and nancy 's sales appearance in japan on behalf of a communications giant and its controversial founder\n",
            "1 the <unk> <unk> spokesman president of chief of the <unk> and the boston of the 's to the the of <unk> the <unk> <unk> court are to be to be N N of and N N of N N N N N N of N N to <start> robert white a vice president and manager of corporate trade at first interstate of california agreed with that view and predicted the u.s. federal funds rate will drop to between N N N and N N within N days from its current level at N N N\n",
            "2 the and at a N N increase profit of in was have the <unk> of of the hanover and said a N N million loss for the quarter ended the N N million in $ <unk> share the to the and <end> <end> <end> <end> <end> <end> <start> citicorp yesterday reported a N N third-quarter earnings drop which analysts called a bit disappointing while manufacturers hanover corp. posted a $ N million loss for the quarter after adding $ N million to its reserve for loans to less-developed countries\n",
            "3 the 's a a the 's a <unk> <unk> of the <unk> of of company york <unk> will to to to be the <unk> to of the <unk> <unk> bond to be <unk> of the N <unk> of $ and <end> <end> <end> <end> <end> <end> <end> <start> it turns out that columbia had this huge loss in large part because the new <unk> mandated rules forced it to adjust the book value of its <unk> junk bonds to the lower of either their cost or market value\n",
            "4 the <unk> of the N shares of <unk> been <unk> of the <unk> <unk> of the 's <unk> of the <unk> crash swiss <unk> <unk> <unk> <unk> been of <unk> 's the <unk> and and <unk> the the the <end> <end> <end> <end> <end> <end> <end> <end> <start> the federation with N million members nationwide has been one of the sharpest critics of exxon 's handling of the N million <unk> tanker spill and has accused the company of repeatedly ignoring requests to meet and discuss it\n",
            "5 the said n't a the <unk> to the <unk> the year 's the <unk> <unk> the 's in the corporations in and <unk> francisco bay the the the a <unk> the <unk> of <unk> <unk> of to be <unk> <end> <end> <end> <end> <end> <end> <end> <end> <start> she has been on the move almost <unk> since last thursday when an army of adjusters employed by major insurers <unk> the san francisco area to help policyholders <unk> through the rubble and restore some order to their lives\n",
            "6 the 'm the <unk> <unk> says the the was n't a <unk> to the <unk> <unk> 's the are be the <unk> <unk> <unk> at by <unk> and in the <unk> in <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> i told my driver he said that he was taking my <unk> to the central committee so they can <unk> <unk> <unk> his hand made vigorous <unk> gestures on his left palm\n",
            "7 the the <unk> 's the <unk> the the <unk> industry and the the <unk> to <unk> <unk> of of the <unk> 's be the <unk> the <unk> and the <unk> in <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> although the network is n't connected to the computer systems that operate either galileo or the shuttle part of the network will carry <unk> of galileo data once the craft gets <unk>\n",
            "8 the <unk> said are out the <unk> 's the <unk> and <unk> the <unk> <unk> be to to <unk> to <unk> at <unk> <unk> <unk> <unk> the 's <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> johnson who worked for mr. corry in strategic planning recalls how his boss would routinely ask a subordinate to research an entire industry to target acquisition candidates\n",
            "9 the of the <unk> N funds are to the <unk> <unk> fund and to N N N million from the third of sept. 's company of of <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> assets of the N taxable funds tracked by <unk> 's money fund report jumped to $ N billion in the week ended tuesday the <unk> <unk> newsletter said\n",
            "10 the <unk> bond are the nabisco and said in at the and the company 's n't to <unk> <unk> in and be and and and the N million <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the junk bonds of rjr nabisco inc. rallied friday on news that the company is selling its candy bar brands to nestle foods corp. for $ N million\n",
            "11 the company 's <unk> is the <unk> account in be <unk> <unk> bit in the is n't than than the <unk> and the <unk> of the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the fed 's case for its own independence would be a little stronger if it were more <unk> of the independence of the rest of the government\n",
            "12 the are be <unk> to average to of said the <unk> <unk> of be the warner the <unk> of trading the <unk> to the <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> advertisers will be offered an advertising package which for a single price will include time on the cnbc program and ad pages in the special <unk>\n",
            "13 the 's company jones industrial average tumbled n't N N with a 's to N to the in a of the N in of the and <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> yesterday the dow jones industrial average did a now familiar dance it plunged N points before lunch with most of the drop <unk> in N minutes\n",
            "14 the <unk> are the <unk> <unk> and said n't by the <unk> of in said he the 're n't to be the of than <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> other makers of video <unk> equipment also were caught in the <unk> shift he said but we were able to respond much more quickly\n",
            "15 the <unk> analysts officials are are that the company <unk> union is <unk> at the the <unk> and the the and than than <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> and some u.s. army analysts worry that the proposed soviet <unk> is aimed at blocking the u.s. from developing lighter more <unk> high-technology tanks\n",
            "16 the the <unk> are are a N million or the to of the 's 's from the N million in the quarter period <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> however the quarter results included $ N million in royalty income from patent licenses up from $ N million in the year-earlier period\n",
            "17 the the <unk> to <unk> <unk> <unk> to be the <unk> of the and the <unk> of to the <unk> a year <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> when properly applied the <unk> is designed to retain the <unk> in place in a crash test at N miles per hour\n",
            "18 the 's n't <unk> that be <unk> and of the <unk> market is to than is be <unk> to <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> there is little incentive to buy gold because if the stock market goes higher it may be just a false alarm\n",
            "19 the <unk> <unk> <unk> and president of chief chief executive officer of the <unk> <unk> said named president vice president <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> scott c. smith formerly vice president finance and chief financial officer of this media concern was named senior vice president\n",
            "20 the the <unk> years to the <unk> <unk> <unk> <unk> <unk> index 's said <unk> of in the and <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> after <unk> three days of heavy selling the beleaguered nasdaq over-the-counter market finally rebounded rising sharply in <unk> trading\n",
            "21 the <unk> co. <unk> a it will to <unk> operations to $ in <unk> $ $ N in the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> motor cars inc. said it expects its u.s. sales to remain steady at about N cars in N\n",
            "22 the of of the example <unk> air 's said that the N of the in the the had be <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> one poll conducted for the british broadcasting corp. found that N N of voters believed that she should quit\n",
            "23 the the <unk> is <unk> <unk> the <unk> of markets of to of the rates be the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> indeed the dlj banker says in the <unk> capital structure cash coverage of interest will <unk> improve\n",
            "24 the in n't to be the the 's <unk> indicators is to in <unk> the <unk> are <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> things were supposed to change when vietnam 's economic reforms gathered pace and for <unk> they did\n",
            "25 the 's <unk> said it the the company 's n't by to for the and the from <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> citicorp 's spokesman said however that the bank is maintaining those expenses in proportion to revenue growth\n",
            "26 the of the <unk> hand and the N of N N N N million <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> imports on the other hand leaped N N to a record $ N billion\n",
            "27 the <unk> <unk> was the <unk> of n't likely of <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> donald vinson who oversees the experiments is n't some <unk> researcher\n",
            "28 the the for been <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> and earnings have been <unk>\n",
            "29 the <unk> <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> inc chandler ariz.\n",
            "30 the <unk> the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> interpublic on tv\n",
            "31 the of <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> odds and ends\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\",\n",
              " \"the company said it expects to sell its <unk> operations in the first half of N million shares outstanding by the company 's <unk> unit of $ N million of N N to $ N million from $ N million in the third quarter compared with a N N stake in the company 's <unk> unit of $ N million\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wscZUx5elol7",
        "colab_type": "text"
      },
      "source": [
        "MMD training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pm4WjRkamMRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = TextVAE(rnn_type='LSTM', loss_type='mmd')\n",
        "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N25faMkkldGi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "outputId": "19879853-1a8c-4601-e857-1b10d15fb757"
      },
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "nb_batches = len(data_train_tokens)//batch_size\n",
        "pad_tok = word2token['<pad>']\n",
        "start_tok = word2token['<start>']\n",
        "end_tok = word2token['<end>']\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    for id_batch in range(nb_batches):\n",
        "        data_batch_as_list = data_train_tokens[(batch_size * id_batch):\n",
        "                                        ((id_batch + 1) * batch_size)]\n",
        "        batch_lengths = [len(seq[:seq.index(pad_tok)]) if pad_tok in seq \n",
        "                              else len(seq) for seq in data_batch_as_list]\n",
        "        batch_lengths = torch.IntTensor(batch_lengths)\n",
        "        sorted_lengths, sorted_idx = torch.sort(batch_lengths, descending=True)\n",
        "        \n",
        "        input_batch = deepcopy(data_batch_as_list)\n",
        "        input_batch = [[start_tok] + seq[:-1] for seq in input_batch]\n",
        "        input_batch = torch.LongTensor(input_batch)\n",
        "        input_batch = input_batch.view(batch_size, max_sequence_length)\n",
        "        input_batch = input_batch[sorted_idx]\n",
        "        \n",
        "        target_batch = deepcopy(data_batch_as_list)\n",
        "        target_batch = [seq[:-1] + [end_tok] if seq[-1] != pad_tok \n",
        "                          else seq[:seq.index(pad_tok)] + [end_tok] + \n",
        "                            seq[seq.index(pad_tok) + 1:] for seq in target_batch]\n",
        "        target_batch = torch.LongTensor(target_batch)\n",
        "        target_batch = target_batch.view(batch_size, max_sequence_length)\n",
        "        target_batch = target_batch[sorted_idx]        \n",
        "        \n",
        "        \n",
        "        latent_var, data_batch_recon = vae(input_batch, sorted_lengths)\n",
        "        \n",
        "        \n",
        "        mmd_loss = vae.loss_mmd(latent_var)\n",
        "        nll_loss = vae.loss_nll(target_batch, data_batch_recon, batch_lengths)\n",
        "        mmd_weight = vae.annealing(id_batch + nb_batches * epoch)\n",
        "        \n",
        "        loss = (nll_loss + mmd_weight * mmd_loss) / batch_size\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if not id_batch % 10:\n",
        "            print(f'Epoch {epoch + 1}/{epochs}, Batch {id_batch}/{nb_batches}, \\\n",
        "             Loss MMD: {mmd_loss}; Loss NLL: {nll_loss}, Anneal: {mmd_weight}')\n",
        "            \n",
        "    # save model at each epoch:\n",
        "    torch.save(vae, 'textVAE_elbo_after_epoch_{}_incorp_reparam.pt'.format(epoch))\n",
        "    # https://stackoverflow.com/questions/52277083/pytorch-saving-model-userwarning-couldnt-retrieve-source-code-for-container-of"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-218783242a79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnll_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmmd_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmmd_loss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mid_batch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm6nX1oxxSIa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1332
        },
        "outputId": "86187725-1335-40fa-bc09-1e48486dd712"
      },
      "source": [
        "print('Model trained with MMD objective stopped at the beginning of the third epoch:')\n",
        "\n",
        "pad_tok = word2token['<pad>']\n",
        "start_tok = word2token['<start>']\n",
        "end_tok = word2token['<end>']\n",
        "\n",
        "from copy import deepcopy\n",
        "data_batch_as_list = data_train_tokens[:batch_size]\n",
        "batch_lengths = [len(seq[:seq.index(pad_tok)]) if pad_tok in seq \n",
        "                      else seq for seq in data_batch_as_list]\n",
        "batch_lengths = torch.IntTensor(batch_lengths)\n",
        "sorted_lengths, sorted_idx = torch.sort(batch_lengths, descending=True)\n",
        "\n",
        "example_batch = deepcopy(data_batch_as_list)\n",
        "example_batch = [[start_tok] + seq[:-1] for seq in example_batch]\n",
        "example_batch = torch.LongTensor(example_batch)\n",
        "example_batch = example_batch.view(batch_size, max_sequence_length)\n",
        "example_batch = example_batch[sorted_idx]\n",
        "vae.train()\n",
        "\n",
        "print('Sentences generated by a forward pass of a batch of data through the VAE:')\n",
        "\n",
        "for i in range(example_batch.size(0)):\n",
        "  print(i, convert_sequence_of_tokens_to_sentence((torch.max((vae(example_batch, sorted_lengths)[-1]), dim=-1)[1][i]).tolist()),\n",
        "          convert_sequence_of_tokens_to_sentence(example_batch[i].tolist()))\n",
        "  \n",
        "print('\\n \\n Sentences greedily generated from the latent code generated by the same batch of data:')\n",
        "list(map(convert_sequence_of_tokens_to_sentence, vae.sample_sentence(z=vae(example_batch, sorted_lengths)[0].squeeze())[1].int()))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model trained with MMD objective stopped at the beginning of the third epoch:\n",
            "Sentences generated by a forward pass of a batch of data through the VAE:\n",
            "0 for the the the <unk> that the have the the a as <unk> <unk> to year been been <unk> <unk> of the the <unk> of the and <unk> to the the <unk> <unk> <unk> in and the 's the and the <unk> <unk> in the <unk> to <start> for journalists however who write what they <unk> view as history 's first draft this has also been a week to give a lot of space and time to ron and nancy 's sales appearance in japan on behalf of a communications giant and its controversial founder\n",
            "1 one <unk> <unk> <unk> president of chief of the <unk> & the boston corp. the 's to the the of the of company company reserve were to be to be N N from to N N in the N N N N N in the N of <start> robert white a vice president and manager of corporate trade at first interstate of california agreed with that view and predicted the u.s. federal funds rate will drop to between N N N and N N within N days from its current level at N N N\n",
            "2 separately said the a loss N N loss of in is said the $ of of the 's corp. 's a N N million in of the quarter of the and N million or $ $ stock the to $ $ <end> <end> <end> <end> <end> <end> <start> citicorp yesterday reported a N N third-quarter earnings drop which analysts called a bit disappointing while manufacturers hanover corp. posted a $ N million loss for the quarter after adding $ N million to its reserve for loans to less-developed countries\n",
            "3 it is the of the 's been year in of the <unk> of the company york <unk> of to to 's be the <unk> of of the <unk> and bonds and the <unk> of the <unk> own of the 's <end> <end> <end> <end> <end> <end> <end> <start> it turns out that columbia had this huge loss in large part because the new <unk> mandated rules forced it to adjust the book value of its <unk> junk bonds to the lower of either their cost or market value\n",
            "4 the company of the million shares of the been <unk> of the <unk> of of the 's <unk> of the <unk> N shares <unk> and and <unk> <unk> of <unk> 's the and to to the the <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <start> the federation with N million members nationwide has been one of the sharpest critics of exxon 's handling of the N million <unk> tanker spill and has accused the company of repeatedly ignoring requests to meet and discuss it\n",
            "5 we said been <unk> the <unk> of to and the week and the <unk> of the and by <unk> <unk> and and <unk> francisco <unk> of the the <unk> and the <unk> 's <unk> <unk> of to <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <start> she has been on the move almost <unk> since last thursday when an army of adjusters employed by major insurers <unk> the san francisco area to help policyholders <unk> through the rubble and restore some order to their lives\n",
            "6 i 'm the <unk> is says he he was n't to <unk> and the <unk> <unk> and <unk> have be the and and <unk> to to <unk> and to the <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> i told my driver he said that he was taking my <unk> to the central committee so they can <unk> <unk> <unk> his hand made vigorous <unk> gestures on his left palm\n",
            "7 despite the <unk> is n't <unk> to be <unk> of and would a <unk> <unk> <unk> <unk> <unk> of the <unk> of be the and the <unk> and a <unk> of of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> although the network is n't connected to the computer systems that operate either galileo or the shuttle part of the network will carry <unk> of galileo data once the craft gets <unk>\n",
            "8 <unk> <unk> <unk> <unk> to the <unk> 's the <unk> and <unk> the <unk> <unk> be the to <unk> of the the <unk> <unk> and the the of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> johnson who worked for mr. corry in strategic planning recalls how his boss would routinely ask a subordinate to research an entire industry to target acquisition candidates\n",
            "9 last of the <unk> N of in in the <unk> <unk> in and that N N N million from the past 's sept. 's company of of of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> assets of the N taxable funds tracked by <unk> 's money fund report jumped to $ N billion in the week ended tuesday the <unk> <unk> newsletter said\n",
            "10 the company bonds were N of N said N 's the N the company 's expected by N N to and $ at and and N N million <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the junk bonds of rjr nabisco inc. rallied friday on news that the company is selling its candy bar brands to nestle foods corp. for $ N million\n",
            "11 the <unk> is <unk> is the <unk> <unk> of be a <unk> <unk> <unk> the is n't than than the <unk> of the <unk> of the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the fed 's case for its own independence would be a little stronger if it were more <unk> of the independence of the rest of the government\n",
            "12 other are be able to <unk> to of is the <unk> <unk> of be the to the <unk> and trading <unk> <unk> <unk> the u.s. <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> advertisers will be offered an advertising package which for a single price will include time on the cnbc program and ad pages in the special <unk>\n",
            "13 as the company jones industrial average of to N N of N 's N N from the 's the of the N of in the N <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> yesterday the dow jones industrial average did a now familiar dance it plunged N points before lunch with most of the drop <unk> in N minutes\n",
            "14 president <unk> <unk> <unk> <unk> and and <unk> <unk> <unk> the <unk> of of said that <unk> 're n't to be the of than <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> other makers of video <unk> equipment also were caught in the <unk> shift he said but we were able to respond much more quickly\n",
            "15 and the of officials are say that the <unk> <unk> union are the at the in <unk> and the the the than and to <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> and some u.s. army analysts worry that the proposed soviet <unk> is aimed at blocking the u.s. from developing lighter more <unk> high-technology tanks\n",
            "16 among the company of N N N million or the sales of $ N and N N N million or N third period <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> however the quarter results included $ N million in royalty income from patent licenses up from $ N million in the year-earlier period\n",
            "17 but the is the <unk> of <unk> to the the <unk> of the of the <unk> of to the N in <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> when properly applied the <unk> is designed to retain the <unk> in place in a crash test at N miles per hour\n",
            "18 there is a <unk> in the the of of the <unk> market is to in is be <unk> to <unk> of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> there is little incentive to buy gold because if the stock market goes higher it may be just a false alarm\n",
            "19 william officials <unk> <unk> <unk> president of and chief executive officer of <unk> <unk> <unk> said named president vice president <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> scott c. smith formerly vice president finance and chief financial officer of this media concern was named senior vice president\n",
            "20 after the the years of the <unk> and <unk> of N trading and and and in for the and <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> after <unk> three days of heavy selling the beleaguered nasdaq over-the-counter market finally rebounded rising sharply in <unk> trading\n",
            "21 <unk> <unk> co. rose said it will to quarterly stock of $ $ by $ N N N N <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> motor cars inc. said it expects its u.s. sales to remain steady at about N cars in N\n",
            "22 one of the in the company government and said that the N of the 's to the would be <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> one poll conducted for the british broadcasting corp. found that N N of voters believed that she should quit\n",
            "23 if the <unk> of the the the u.s. of markets and <unk> in the rates be the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> indeed the dlj banker says in the <unk> capital structure cash coverage of interest will <unk> improve\n",
            "24 many are n't to be the the of <unk> <unk> is that and <unk> the <unk> are <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> things were supposed to change when vietnam 's economic reforms gathered pace and for <unk> they did\n",
            "25 first 's <unk> said the the the company 's n't by at and the to <unk> the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> citicorp 's spokesman said however that the bank is maintaining those expenses in proportion to revenue growth\n",
            "26 for for the new hand the N N of N N N N million <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> imports on the other hand leaped N N to a record $ N billion\n",
            "27 one <unk> <unk> is <unk> <unk> of <unk> <unk> of <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> donald vinson who oversees the experiments is n't some <unk> researcher\n",
            "28 and the were been <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> and earnings have been <unk>\n",
            "29 <unk> <unk> <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> inc chandler ariz.\n",
            "30 new 's the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> interpublic on tv\n",
            "31 <unk> <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> odds and ends\n",
            "\n",
            " \n",
            " Sentences greedily generated from the latent code generated by the same batch of data:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['for the past few years ago the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>',\n",
              " \"one of the company 's <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> & co. 's <unk> <unk> unit of $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a\",\n",
              " 'separately <unk> corp. said it expects to $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share',\n",
              " \"it 's a <unk> <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of\",\n",
              " \"the company said it will be a <unk> of the <unk> of the <unk> of the company 's <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\",\n",
              " \"we 're not to be a <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\",\n",
              " \"i 'm not going to be <unk> to the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk>\",\n",
              " 'yet the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " '<unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " \"sales of the company 's <unk> <unk> unit of $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents\",\n",
              " \"the company said it will be sold by the company 's <unk> unit of $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $\",\n",
              " 'the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " \"other companies are n't <unk> by the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>\",\n",
              " \"as a result of the company 's <unk> <unk> unit was quoted at $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N\",\n",
              " 'american express <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and',\n",
              " 'and the <unk> of the <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " \"among the nine months of N N of the N N <unk> <unk> <unk> <unk> <unk> and <unk> <unk> corp. said it will be sold by the company 's <unk> unit of $ N million or N cents a share from $ N million or $ N a share from $ N million or N cents a share from $\",\n",
              " 'as a result of the <unk> of the <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " 'there is a <unk> of the <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and',\n",
              " 'william <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " \"after the u.s. and the company 's <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\",\n",
              " \"<unk> corp. said it will be sold by the company 's <unk> unit of $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $\",\n",
              " \"one of the <unk> of the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> inc. said it will be sold by the company 's <unk> unit of $ N million or N cents a share from $ N million or N cents a share from $ N million\",\n",
              " 'if the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>',\n",
              " 'one reason to the <unk> of the <unk> of the <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and',\n",
              " 'first boston corp. said it will be <unk> by the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of',\n",
              " 'for the nine months ended sept. N to $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a share from $ N million or N cents a',\n",
              " 'one thing that the <unk> of the <unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> and',\n",
              " 'and the company said it will be a <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk>',\n",
              " '<unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>',\n",
              " \"new york city 's <unk> <unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\",\n",
              " '<unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWQUERUorrDZ",
        "colab_type": "code",
        "outputId": "16615003-1030-4c85-d528-f9d2d24457e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        }
      },
      "source": [
        "# READ: https://arxiv.org/abs/1402.0030  https://www.reddit.com/r/MachineLearning/comments/46xjtw/how_to_understand_the_kl_divergence_term_in/ \n",
        "# annealing function where it comes from?\n",
        "#[i for i in list(torch.max((model(input_batch, sorted_lengths)[-1]), dim=-1)[1])]\n",
        "for i in range(input_batch.size(0)):\n",
        "  print(i, convert_sequence_of_tokens_to_sentence((torch.max((model(input_batch, sorted_lengths)), dim=-1)[1][i]).tolist()),\n",
        "          convert_sequence_of_tokens_to_sentence(input_batch[i].tolist()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 the the interview <unk> the <unk> 's been in a <unk> <unk> in <unk> <unk> of <unk> <unk> president of a. <unk> of a to <unk> <unk> <unk> <unk> of the 's in and N of the a <unk> $ of the $ <unk> of the <unk> market the and <start> in an <unk> indictment the government has charged gaf a wayne n.j. specialty chemical maker and its vice chairman james t. sherwin with attempting to manipulate the common stock of union carbide corp. in advance of gaf 's planned sale of a large block of the stock in november N\n",
            "1 the <unk> of <unk> <unk> <unk> been to for the <unk> <unk> in the <unk> <unk> <unk> <unk> <unk> week the years ago the and <unk> <unk> the the N old the and and the <unk> years and and the <unk> 's <end> <end> <end> <end> <end> <end> <end> <end> <start> dallas district judge jack <unk> had sparked calls for a judicial inquiry with his remarks to the press last december two weeks after sentencing an <unk> defendant to N years in state prison for killing two homosexual men in a city park\n",
            "2 the the <unk> <unk> a <unk> <unk> of to <unk> <unk> <unk> court in the <unk> a n't have of <unk> <unk> to the <unk> the says n't the <unk> 's <unk> <unk> in the 's <unk> the <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> but robert r. murray a special master appointed by the texas supreme court said judge <unk> did n't breach any judicial standards of fairness although he did violate the state 's judicial code by commenting publicly on a pending case\n",
            "3 the <unk> spokesman the and <unk> <unk> concern said a the and in <unk> of the <unk> of <unk> are the and the in <unk> 's the a <unk> <unk> as <unk> <unk> <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> a department of health and human services rule adopted in N prohibits the use of so-called title x funds for programs that assist a woman in obtaining an abortion such as abortion counseling and <unk>\n",
            "4 the <unk> <unk> and a <unk> <unk> of and the 's germany the N a in and 's a <unk> to <unk> the <unk> <unk> to the and <unk> <unk> <unk> of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> <unk> trudeau is suing the writers guild of america east for $ N million alleging it mounted a campaign to <unk> and punish him for crossing a <unk> ' picket line\n",
            "5 the <unk> 's that the <unk> are n't have the <unk> to the <unk> the and and and <unk> the <unk> of the <unk> and n't have the <unk> of the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the panel ruled that the restrictions do n't violate the freedom of speech of health care <unk> and that the limits on counseling services do n't violate the rights of pregnant women\n",
            "6 the <unk> <unk> <unk> at a to the <unk> of a as <unk> that are have be to <unk> in they are to been a to <unk> of in to in <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the judge was quoted as referring to the victims as <unk> and saying they would n't have been killed if they had n't been <unk> the streets picking up <unk> boys\n",
            "7 the <unk> the <unk> 's been been the of in <unk> the <unk> 's that the was be <unk> to the <unk> to <unk> <unk> <unk> <unk> <unk> a <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> that the judge has never <unk> any bias or <unk> mr. murray concluded that he would be <unk> in any case involving a homosexual or <unk> as a victim\n",
            "8 the <unk> <unk> have <unk> <unk> <unk> <unk> of <unk> funds of the <unk> and to the <unk> of the <unk> consecutive and <unk> in <unk> <unk> the york <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> title x funds are the single largest source of federal funding for <unk> services according to the opinion by the second u.s. circuit court of appeals in new york\n",
            "9 the <unk> reserve court is that <unk> <unk> in that the <unk> is be <unk> <unk> of the <unk> and the <unk> in the the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> a federal appeals court upheld a lower court ruling that the u.s. can bar the use of federal funds for <unk> programs that include <unk> services\n",
            "10 the the <unk> york is a of the of the funds and the <unk> and in the <unk> <unk> <unk> said said be a of of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> at his new job as partner in charge of federal litigation in the sacramento office of <unk> <unk> & <unk> he will make out much better\n",
            "11 the <unk> the <unk> quarter of of the the and a the in by the u.s. 's and the trade of <unk> <unk> said <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> attorneys in the third <unk> trial of gaf corp. began opening arguments yesterday in the manhattan courtroom of u.s. district judge mary johnson lowe\n",
            "12 the <unk> said was be to the <unk> <unk> 's the <unk> 's <unk> to <unk> the <unk> of the <unk> 's be <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> mr. <unk> who will go before the disciplinary panel said the proceedings are unfair and that any punishment from the guild would be <unk>\n",
            "13 the the <unk> <unk> <unk> 's he <unk> is the a in <unk> <unk> of to <unk> the <unk> of the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> in his lawsuit mr. trudeau says the strike illegally included <unk> and the <unk> refused to honor the strike against the company\n",
            "14 the <unk> 's <unk> of 's of said <unk> by a <unk> of the <unk> is <unk> by the <unk> <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> mr. trudeau a writers guild member also was employed as a writer for <unk> which was covered by a guild <unk> agreement\n",
            "15 the the the the <unk> of <unk> is to <unk> in to a <unk> of the <unk> <unk> <unk> the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> in addition to the damages the suit seeks a court order preventing the guild from <unk> or <unk> against mr. trudeau\n",
            "16 the <unk> 's <unk> for <unk> <unk> <unk> the <unk> <unk> of for the <unk> <unk> <unk> <unk> of <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> mr. trudeau 's attorney norman k. <unk> said the <unk> consists mainly of the guild 's <unk> threats of disciplinary action\n",
            "17 the <unk> <unk> to comment the own of the that the it do not to be <unk> <unk> <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the judge declined to discuss his salary in detail but said i 'm going to be a high-priced lawyer\n",
            "18 the <unk> was n't to be the the <unk> 's 's the <unk> to <unk> <unk> to the a <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the report is subject to review by the state commission on judicial conduct which is <unk> to impose sanctions\n",
            "19 the said the <unk> to <unk> to <unk> to comment on the the <unk> <unk> <unk> in the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> he said disciplinary proceedings are confidential and declined to comment on whether any are being held against mr. trudeau\n",
            "20 the <unk> 's said the <unk> a <unk> of n't have the <unk> <unk> <unk> <unk> 's the <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> mr. murray also said judge <unk> 's comments did n't <unk> the judiciary or the administration of justice\n",
            "21 the <unk> is <unk> <unk> of and <unk> <unk> of said the the <unk> 's a <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the dispute involves <unk> productions inc. a tv production company in which mr. trudeau is a <unk>\n",
            "22 the <unk> said the <unk> <unk> <unk> in a to year in the york <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> mr. <unk> said a guild disciplinary hearing is scheduled next monday in new york\n",
            "23 the <unk> 's to <unk> in the <unk> <unk> <unk> <unk> is the N <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the guild began a strike against the tv and movie industry in march N\n",
            "24 the <unk> for the <unk> 's the <unk> 's <unk> have <unk> the <unk> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> a spokesman for the guild said the union 's lawyers are reviewing the suit\n",
            "25 the <unk> is has <unk> for the of the <unk> and <unk> are <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the rule also prohibits funding for activities that encourage promote or advocate abortion\n",
            "26 the and in and <unk> <unk> 's the of the the and <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> inquiry clears texas judge of bias in comments on homosexual murder victims\n",
            "27 the <unk> time years <unk> is sept. the and this year <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> the first two gaf trials ended in <unk> earlier this year\n",
            "28 the <unk> <unk> <unk> and <unk> <unk> of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> <unk> <unk> union troubles are no laughing matter\n",
            "29 the is is a to be year years <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> this trial is expected to last five weeks\n",
            "30 the N in to be of <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> gaf trial goes to round three\n",
            "31 the <unk> have <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <end> <start> abortion ruling upheld\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Au0-2hK2EA0",
        "colab_type": "code",
        "outputId": "95995b5d-ea40-4153-b7c0-535d5df45a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "convert_sequence_of_tokens_to_sentence(vae.sample_sentence()[1].squeeze().int().tolist())"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this year is a <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GuvJvtHy7xT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "outputId": "45916841-1edf-4193-d571-8072aab5b901"
      },
      "source": [
        "ls_gen_sentences_tokens = vae.sample_sentence(z=torch.randn(4, 2* z_dim))[1].squeeze().int().tolist()\n",
        "\n",
        "for ls_tokens in ls_gen_sentences_tokens:\n",
        "  print(convert_sequence_of_tokens_to_sentence(ls_tokens))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is n't <unk> by the <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\n",
            "people who have been <unk> by the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> <unk> and <unk> <unk> <unk>\n",
            "william <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>\n",
            "that 's <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTAuTVgWzsCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "6bdcf377-dfa1-49d1-f3cf-d7c3c806512e"
      },
      "source": [
        "ls_gen_sentences_tokens = vae.sample_sentence(nb_examples=7)[1].squeeze().int().tolist()\n",
        "\n",
        "for ls_tokens in ls_gen_sentences_tokens:\n",
        "  print(convert_sequence_of_tokens_to_sentence(ls_tokens))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "it is n't likely to be able to be <unk> by the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and\n",
            "and the <unk> of the <unk> of the <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk>\n",
            "<unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\n",
            "the company said it will be a <unk> of the <unk> of the <unk> of the company 's <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\n",
            "new york city 's <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>\n",
            "one of the <unk> of the <unk> <unk> <unk> <unk> <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk>\n",
            "there is a <unk> of the <unk> of the <unk> of the <unk> of the <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and <unk> <unk> and\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}